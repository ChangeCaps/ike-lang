import lex::token

import std::string
import std::io
import std::list

fn main {
  let in, out = std::channel

  std::spawn <| |_| emit out

  let file = file {
    path: "test/parse/expr.ike"
  }

  let input = std::fs::read "test/parse/expr.ike"
    |> std::result::assert

  let result = lex::tokenize in file input
  result |> list::map |t, _| debug-token t 0
}

fn debug-token token indent {
  " " |> string::repeat indent |> io::print

  match token {
    token::group group   -> {
      io::println "group `{group.delimiter}`" 

      group.contents |> list::map |t, _| debug-token t (indent + 2)

      {}
    }

    token::format parts  -> {
      io::println "format"

      parts |> list::map |p| {
        match p {
          token::string s -> {
            " " |> string::repeat (indent + 2) |> io::print
            io::println "format-string '{s}'"
          }

          token::tokens ts -> {
            " " |> string::repeat (indent + 2) |> io::print
            io::println "format-tokens"

            ts |> list::map |t, _| debug-token t (indent + 4)

            {}
          }
        }
      }

      {}
    }

    token::symbol symbol -> io::println "symbol `{symbol}`"
    token::ident ident   -> io::println "ident `{ident}`"
    token::integer s     -> io::println "integer `{s}`"
    token::whitespace    -> io::println "whitespace"
    token::newline       -> io::println "newline"
    token::error         -> io::println "error"
  }
}

fn emit out {
  std::recv out |> std::debug::print
  emit out
}
