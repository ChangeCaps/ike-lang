import std::string
import std::list
import std::option
import std::some
import std::none

import ike::file
import ike::diagnostic

fn tokenize : file -> str -> token::stream, [diagnostic]
fn tokenize file input {
  let lexer = lexer {
    diagnostics: []
    graphs:      string::graphemes input  
    file:        file
    offset:      0
    delim:       none
  }

  let ts, lexer = lexer::lex [] lexer
  list::reverse ts, list::reverse lexer.diagnostics
}

type lexer = {
  diagnostics: [diagnostic]
  graphs:      [str]
  file:        file
  offset:      int
  delim:       option (token::delimiter, span)
}

fn lexer::with-graphs graphs lexer {
  lexer {
    diagnostics: lexer.diagnostics
    graphs:      graphs
    file:        lexer.file
    offset:      lexer.offset
    delim:       lexer.delim
  }
}

fn lexer::with-diagnostic diagnostic lexer {
  lexer {
    diagnostics: [diagnostic; ..lexer.diagnostics]
    graphs:      lexer.graphs
    file:        lexer.file
    offset:      lexer.offset
    delim:       lexer.delim
  }
}

fn lexer::with-diagnostics diagnostics lexer {
  lexer {
    diagnostics: list::append diagnostics lexer.diagnostics
    graphs:      lexer.graphs
    file:        lexer.file
    offset:      lexer.offset
    delim:       lexer.delim
  }
}

fn lexer::with-offset offset lexer {
  lexer {
    diagnostics: lexer.diagnostics
    graphs:      lexer.graphs
    file:        lexer.file
    offset:      lexer.offset + offset
    delim:       lexer.delim
  }
}

fn lexer::with-delim delim lexer {
  lexer {
    diagnostics: lexer.diagnostics
    graphs:      lexer.graphs
    file:        lexer.file
    offset:      lexer.offset
    delim:       delim
  }
}

fn lexer::lex : token::stream -> lexer -> token::stream, lexer
fn lexer::lex ts lexer {
  match lexer.graphs {
    []        -> {
      match lexer.delim {
        none               -> ts, lexer
        some (delim, span) -> {
          let diagnostic = diagnostic::error "expected closing {delim}"
            |> diagnostic::with-label span "here"

          let lexer = lexer
            |> lexer::with-diagnostic diagnostic

          ts, lexer
        }
      }  
    }

    [g; ..gs] -> {
      lexer
        |> lexer::with-graphs gs
        |> lexer::graph ts g
    }
  }
}

fn lexer::graph ts g lexer {
  lexer |> (match g {
    "\n" -> lexer::newline ts g

    " "  -> lexer::whitespace ts g

    "\"" -> lexer::string ts [] "" <| string::len g

    ";"  -> lexer::symbol ts g token::semi
    ":"  -> lexer::symbol ts g token::colon
    ","  -> lexer::symbol ts g token::comma
    "."  -> lexer::symbol ts g token::dot
    "#"  -> lexer::symbol ts g token::pound
    "_"  -> lexer::symbol ts g token::under
    "+"  -> lexer::symbol ts g token::plus
    "-"  -> lexer::symbol ts g token::minus
    "*"  -> lexer::symbol ts g token::star
    "/"  -> lexer::symbol ts g token::slash
    "\\" -> lexer::symbol ts g token::backslash
    "%"  -> lexer::symbol ts g token::percent
    "&"  -> lexer::symbol ts g token::amp
    "|"  -> lexer::symbol ts g token::pipe
    "^"  -> lexer::symbol ts g token::caret
    "!"  -> lexer::symbol ts g token::bang
    "?"  -> lexer::symbol ts g token::question
    "'"  -> lexer::symbol ts g token::quote
    "="  -> lexer::symbol ts g token::eq
    "~"  -> lexer::symbol ts g token::tilde
    "<"  -> lexer::symbol ts g token::lt
    ">"  -> lexer::symbol ts g token::gt

    "0"  -> lexer::integer ts g
    "1"  -> lexer::integer ts g
    "2"  -> lexer::integer ts g
    "3"  -> lexer::integer ts g
    "4"  -> lexer::integer ts g
    "5"  -> lexer::integer ts g
    "6"  -> lexer::integer ts g
    "7"  -> lexer::integer ts g
    "8"  -> lexer::integer ts g
    "9"  -> lexer::integer ts g

    "("  -> lexer::start-group ts g token::parentheses
    "["  -> lexer::start-group ts g token::bracket
    "{"  -> lexer::start-group ts g token::brace

    ")"  -> lexer::end-group ts g token::parentheses
    "]"  -> lexer::end-group ts g token::bracket
    "}"  -> lexer::end-group ts g token::brace

    _    -> {
      match lexer::is-ident-start g {
        true  -> lexer::ident ts g
        false -> lexer::unexpected-graph ts g
      } 
    }
  })
}

fn lexer::whitespace ts g {
  lexer::lex-with
    <| ts
    <| token::whitespace
    <| string::len g
}

fn lexer::newline ts g {
  lexer::lex-with
    <| ts
    <| token::newline
    <| string::len g
}

fn lexer::start-group ts g delim lexer {
  let delim-span = span {
    file:  lexer.file
    start: lexer.offset
    end:   lexer.offset + string::len g
  }
  
  let ts', lexer' = lexer
    |> lexer::with-offset <| string::len g
    |> lexer::with-delim <| some (delim, delim-span)
    |> lexer::lex []

  let span = span {
    file:  lexer.file
    start: lexer.offset
    end:   lexer'.offset
  }

  let group = token::group {
    delimiter: delim
    contents:  ts' |> list::reverse
  }

  let ts = [token::group group, span; ..ts]
  let lexer = lexer' 
    |> lexer::with-delim lexer.delim
    |> lexer::with-diagnostics lexer.diagnostics

  lexer::lex ts lexer
}

fn lexer::end-group ts g delim lexer {
  match lexer.delim |> option::is-some-and (|d, _| d == delim) {
    true  -> ts, (lexer |> lexer::with-offset <| string::len g)
    false -> {
      let span = span {
        file:  lexer.file
        start: lexer.offset
        end:   lexer.offset + string::len g
      }

      let diagnostic = diagnostic::error "unexpected closing {delim}"
        |> diagnostic::with-label span "found here"

      let lexer = lexer
        |> lexer::with-offset <| string::len g
        |> lexer::with-diagnostic diagnostic

      let ts = [token::error, span; ..ts]

      lexer::lex ts lexer
    }
  }
}

fn lexer::string ts parts s len lexer {
  match lexer::peek lexer {
    some "\"" -> {
      lexer::lex-with
        <| ts
        <| token::format (list::reverse [token::string s; ..parts])
        <| len + string::len "\""
        <| lexer::skip lexer
    }

    some "\\" -> {
      let lexer = lexer |> lexer::skip

      let escape = |esc| {
        lexer::string
          <| ts
          <| parts
          <| string::append esc s
          <| len + string::len "\\" + string::len esc
          <| lexer::skip lexer
      }

      match lexer::peek lexer {
        some "\\" -> escape "\\"
        some "\"" -> escape "\""
        some "n"  -> escape "\n"
        some "t"  -> escape "\t"
        some "r"  -> escape "\r"

        some g -> {
          let span = span {
            file:  lexer.file
            start: lexer.offset + len
            end:   lexer.offset + len + string::len "\\" + string::len g
          }

          let diagnostic = diagnostic::error "invalid escape character `{g}`"
            |> diagnostic::with-label span "found here"

          let lexer = lexer
            |> lexer::with-diagnostic diagnostic
            |> lexer::with-offset <| len + string::len "\\" + string::len g

          let ts = [token::format [token::string s], span; ..ts]

          ts, lexer
        }

        none -> {          
          let span = span {
            file:  lexer.file
            start: lexer.offset
            end:   lexer.offset + len
          }

          let diagnostic = diagnostic::error "expected escape character"
            |> diagnostic::with-label span "found here"

          let lexer = lexer
            |> lexer::with-diagnostic diagnostic
            |> lexer::with-offset <| len + string::len "\\"

          let ts = [token::format [token::string s], span; ..ts]

          ts, lexer
        }
      }
    }

    some "{{" -> {
      let lexer = lexer::skip lexer

      match lexer::peek lexer == some "{{" {
        true  -> {
          lexer::string
            <| ts
            <| parts
            <| string::append "{{" s
            <| len + string::len "{{" * 2
            <| lexer::skip lexer
        }

        false -> {
          let delim-span = span {
            file:  lexer.file
            start: lexer.offset + len
            end:   lexer.offset + len + string::len "{{"
          }

          let lexer' = lexer
            |> lexer::with-delim <| some (token::brace, delim-span)
            |> lexer::with-offset <| len + string::len "{{"

          let ts', lexer' = lexer::lex [] lexer'

          let lexer = lexer
            |> lexer::with-graphs lexer'.graphs
            |> lexer::with-diagnostics lexer'.diagnostics

          lexer::string
            <| ts
            <| [token::tokens ts'; token::string s; ..parts]
            <| ""
            <| lexer'.offset - lexer.offset
            <| lexer
        }
      }
    }

    some g -> {
      lexer::string
        <| ts
        <| parts
        <| string::append g s
        <| len + string::len g
        <| lexer::skip lexer
    }

    none -> {
      let span = span {
        file:  lexer.file
        start: lexer.offset
        end:   lexer.offset + len
      }

      let diagnostic = diagnostic::error "expected end of string"
        |> diagnostic::with-label span "found here"

      let lexer = lexer
        |> lexer::with-diagnostic diagnostic
        |> lexer::with-offset len

      let ts = [token::format [token::string s], span; ..ts]

      ts, lexer
    }
  }
}

fn lexer::symbol ts g symbol lexer {
  lexer |> (match symbol, lexer::peek lexer {
    token::dot,   some "." -> lexer::two-graph-symbol ts g "." token::dotdot
    token::minus, some ">" -> lexer::two-graph-symbol ts g ">" token::rarrow
    token::lt,    some "<" -> lexer::two-graph-symbol ts g "<" token::larrow
    token::colon, some ":" -> lexer::two-graph-symbol ts g ":" token::coloncolon
    token::eq,    some "=" -> lexer::two-graph-symbol ts g "=" token::eqeq
    token::bang,  some "=" -> lexer::two-graph-symbol ts g "=" token::noteq
    token::lt,    some "=" -> lexer::two-graph-symbol ts g "=" token::lteq
    token::gt,    some "=" -> lexer::two-graph-symbol ts g "=" token::gteq
    token::lt,    some "|" -> lexer::two-graph-symbol ts g "|" token::ltpipe
    token::pipe,  some ">" -> lexer::two-graph-symbol ts g ">" token::pipegt
    _                      -> lexer::one-graph-symbol ts g symbol
  })
}

fn lexer::two-graph-symbol ts g1 g2 symbol lexer {
  let span = span {
    file:  lexer.file
    start: lexer.offset
    end:   lexer.offset + string::len g1 + string::len g2
  }

  let lexer = lexer
    |> lexer::skip
    |> lexer::with-offset <| string::len g1 + string::len g2

  let ts = [token::symbol symbol, span; ..ts]

  lexer::lex ts lexer
}

fn lexer::one-graph-symbol ts g symbol lexer {
  let span = span {
    file:  lexer.file
    start: lexer.offset
    end:   lexer.offset + string::len g
  }

  let lexer = lexer
    |> lexer::with-offset <| string::len g

  let ts = [token::symbol symbol, span; ..ts]

  lexer::lex ts lexer
}

fn lexer::ident ts s lexer {
  let is-continue = lexer
    |> lexer::peek
    |> option::is-some-and lexer::is-ident-continue

  match is-continue {
    true -> {
      let g = lexer
        |> lexer::peek
        |> option::assert

      let s = s |> string::append g
      let lexer = lexer |> lexer::skip

      lexer::ident ts s lexer
    }

    false -> {
      lexer::lex-with
        <| ts 
        <| token::ident s
        <| string::len s
        <| lexer
    }
  }
}

fn lexer::integer ts s lexer {
  let is-digit = lexer
    |> lexer::peek
    |> option::is-some-and lexer::is-digit

  match is-digit {
    true -> {
      let g = lexer
        |> lexer::peek
        |> option::assert

      let s = s |> string::append g
      let lexer = lexer |> lexer::skip

      lexer::integer ts s lexer
    }

    false -> {
      lexer::lex-with
        <| ts 
        <| token::integer s
        <| string::len s
        <| lexer
    }
  }
}

fn lexer::lex-with ts tok len lexer {
  let span = span {
    file:  lexer.file
    start: lexer.offset
    end:   lexer.offset + len
  }

  let lexer = lexer |> lexer::with-offset len
  let ts = [tok, span; ..ts]

  lexer::lex ts lexer
}

fn lexer::unexpected-graph ts g lexer {
  let span = span {
    file:  lexer.file
    start: lexer.offset
    end:   lexer.offset + string::len g
  }

  let diagnostic = diagnostic::error "unexpected character `{g}`"
    |> diagnostic::with-label span "found here"

  let lexer = lexer
    |> lexer::with-diagnostic diagnostic
    |> lexer::with-offset <| string::len g

  let ts = [token::error, span; ..ts]

  lexer::lex ts lexer
}

fn lexer::peek lexer {
  match lexer.graphs {
    [g; ..] -> some g
    []      -> none
  }
}

fn lexer::skip lexer {
  match lexer.graphs {
    [_; ..gs] -> lexer |> lexer::with-graphs gs
    []        -> lexer
  }
}

fn lexer::is-digit c {
  match c {
    "0" -> true
    "1" -> true
    "2" -> true
    "3" -> true
    "4" -> true
    "5" -> true
    "6" -> true
    "7" -> true
    "8" -> true
    "9" -> true
    _   -> false
  }
}

fn lexer::is-ident-start c {
  "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"
    |> string::graphemes
    |> list::contains c
}

fn lexer::is-ident-continue c {
  lexer::is-ident-start c or lexer::is-digit c or c == "_" or c == "-" or c == "'"
}
